{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('E:/prepare for graduate school/Python/Kaggel/Travelers Competition/uconn_comp_2018_train.csv')\n",
    "test_df = pd.read_csv('E:/prepare for graduate school/Python/Kaggel/Travelers Competition/uconn_comp_2018_test.csv')\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See first 5 rows and all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See column data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See how many rows and columns are in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See how many rows are left after we drop rows that contains missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We loose 162 rows of data, 0.9% of data. Not a big lose\n",
    "#### We can see how many missing value we have in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For categorical variables, we can change their type to category and save memory(not necessary)\n",
    "\n",
    "we can look at how much memory are they using now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.living_status.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['living_status'] = train_df.living_status.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.claim_day_of_week.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['claim_day_of_week'] = train_df.claim_day_of_week.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.accident_site.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['accident_site'] = train_df.accident_site.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we are not having a huge dataset, we will leave the following first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.channel.memory_usage()\n",
    "train_df.vehicle_category.memory_usage()\n",
    "train_df.vehicle_color.memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze by pivoting features and visualizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.loc[train_df['fraud']!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train_df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(train_df,col='fraud')\n",
    "grid.map(plt.hist,'age_of_driver',bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['gender','fraud']].groupby(['gender'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['marital_status','fraud']].groupby(['marital_status'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(train_df,col='fraud')\n",
    "grid.map(plt.hist,'safty_rating',bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(train_df,col='fraud')\n",
    "grid.map(plt.hist,'annual_income',bins=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['high_education_ind','fraud']].groupby(['high_education_ind'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['address_change_ind','fraud']].groupby(['address_change_ind'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['living_status','fraud']].groupby(['living_status'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['claim_date','fraud']].groupby(['claim_date'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['claim_day_of_week','fraud']].groupby(['claim_day_of_week'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['accident_site','fraud']].groupby(['accident_site'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['past_num_of_claims','fraud']].groupby(['past_num_of_claims'],as_index=False).mean().sort_values('past_num_of_claims')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['witness_present_ind','fraud']].groupby(['witness_present_ind'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(train_df,col='fraud')\n",
    "grid.map(plt.hist,'liab_prct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['channel','fraud']].groupby(['channel'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Almost no difference, can be excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['policy_report_filed_ind','fraud']].groupby(['policy_report_filed_ind'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['age_of_vehicle','fraud']].groupby(['age_of_vehicle'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['vehicle_category','fraud']].groupby(['vehicle_category'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(train_df,col='fraud')\n",
    "grid.map(plt.hist,'vehicle_price',bins=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['vehicle_price','fraud']].groupby(['vehicle_price'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['vehicle_color','fraud']].groupby(['vehicle_color'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(train_df,col='fraud')\n",
    "grid.map(plt.hist,'vehicle_weight',bins=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['vehicle_weight','fraud']].groupby(['vehicle_weight'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference is very small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### age_of_driver\n",
    "Replace ages over 100 with mean value calculated by age under or equals 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df.age_of_driver <= 100].age_of_driver.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df.age_of_driver > 100,'age_of_driver'] = 43\n",
    "\n",
    "train_df['DriverAgeBand'] = pd.cut(train_df['age_of_driver'], 7)\n",
    "train_df[['DriverAgeBand', 'fraud']].groupby(['DriverAgeBand'], as_index=False).mean().sort_values(by='DriverAgeBand', ascending=True)\n",
    "\n",
    "for dataset in combine:    \n",
    "    dataset.loc[ dataset['age_of_driver'] <= 30, 'age_of_driver'] = 0\n",
    "    dataset.loc[(dataset['age_of_driver'] > 30) & (dataset['age_of_driver'] <= 41), 'age_of_driver'] = 1\n",
    "    dataset.loc[(dataset['age_of_driver'] > 41) & (dataset['age_of_driver'] <= 53), 'age_of_driver'] = 2\n",
    "    dataset.loc[(dataset['age_of_driver'] > 53) & (dataset['age_of_driver'] <= 64), 'age_of_driver'] = 3\n",
    "    dataset.loc[(dataset['age_of_driver'] > 64) & (dataset['age_of_driver'] <= 76), 'age_of_driver'] = 4\n",
    "    dataset.loc[(dataset['age_of_driver'] > 76) & (dataset['age_of_driver'] <= 88), 'age_of_driver'] = 5\n",
    "    dataset.loc[ dataset['age_of_driver'] > 88, 'age_of_driver'] = 6\n",
    "    \n",
    "train_df = train_df.drop(['DriverAgeBand'], axis=1)\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['age_of_driver','fraud']].groupby(['age_of_driver'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.gender = train_df.gender.map({'M':1,'F':0})\n",
    "test_df.gender = test_df.gender.map({'M':1,'F':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### marital_status\n",
    "\n",
    "Because marital_status is a 1/0, it is hard for us to impute, and there are only 5 rows of missing value, so we will simply delete those rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(subset=['marital_status'],inplace=True)\n",
    "test_df.dropna(subset=['marital_status'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safty_rating, annual_income, high_education_ind, address_change_ind\n",
    "These columns are good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.safty_rating.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['SaftyBand'] = pd.cut(train_df['safty_rating'], 5)\n",
    "train_df[['SaftyBand', 'fraud']].groupby(['SaftyBand'], as_index=False).mean().sort_values(by='SaftyBand', ascending=True)\n",
    "\n",
    "for dataset in combine:    \n",
    "    dataset.loc[ dataset['safty_rating'] <= 20, 'safty_rating'] = 0\n",
    "    dataset.loc[(dataset['safty_rating'] > 20) & (dataset['safty_rating'] <= 40), 'safty_rating'] = 1\n",
    "    dataset.loc[(dataset['safty_rating'] > 40) & (dataset['safty_rating'] <= 60), 'safty_rating'] = 2\n",
    "    dataset.loc[(dataset['safty_rating'] > 60) & (dataset['safty_rating'] <= 80), 'safty_rating'] = 3\n",
    "    dataset.loc[dataset['safty_rating'] > 80 , 'safty_rating'] = 4\n",
    "    \n",
    "train_df = train_df.drop(['SaftyBand'], axis=1)\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['safty_rating','fraud']].groupby(['safty_rating'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### annual_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df.annual_income > 0].annual_income.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df.annual_income < 0,'annual_income'] = 37398\n",
    "\n",
    "train_df.loc[:,'annual_income_band'] = pd.cut(train_df['annual_income'], 5)\n",
    "train_df[['annual_income_band', 'fraud']].groupby(['annual_income_band'], as_index=False).mean().sort_values(by='annual_income_band', ascending=True)\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset.loc[ dataset['annual_income'] <= 28895, 'annual_income'] = 0\n",
    "    dataset.loc[(dataset['annual_income'] > 28895) & (dataset['annual_income'] <= 34972), 'annual_income'] = 1\n",
    "    dataset.loc[(dataset['annual_income'] > 34972) & (dataset['annual_income'] <= 36890), 'annual_income'] = 2\n",
    "    dataset.loc[(dataset['annual_income'] > 36890) & (dataset['annual_income'] <= 38282), 'annual_income'] = 3\n",
    "    dataset.loc[(dataset['annual_income'] > 38282) & (dataset['annual_income'] <= 39707), 'annual_income'] = 4\n",
    "    dataset.loc[ dataset['annual_income'] > 39707, 'annual_income'] = 5\n",
    "    dataset['annual_income'] = dataset['annual_income'].astype(int)\n",
    "\n",
    "train_df = train_df.drop(['annual_income_band'], axis=1)\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Living_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.living_status = train_df.living_status.map({'Own':1,'Rent':0})\n",
    "test_df.living_status = test_df.living_status.map({'Own':1,'Rent':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip_code is a little hard for me right now, just leave it there first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop('zip_code',axis=1,inplace=True)\n",
    "test_df.drop('zip_code',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claim_date\n",
    "Extract month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.claim_date = pd.to_datetime(train_df.claim_date).dt.month\n",
    "test_df.claim_date = pd.to_datetime(test_df.claim_date).dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### claim_day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.claim_day_of_week = pd.to_datetime(train_df.claim_date).dt.dayofweek\n",
    "test_df.claim_day_of_week = pd.to_datetime(test_df.claim_date).dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accident_site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.accident_site = train_df.accident_site.map({'Local':1,'Parking Lot':2,'Highway':3})\n",
    "test_df.accident_site = test_df.accident_site.map({'Local':1,'Parking Lot':2,'Highway':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### witness_present_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.witness_present_ind = train_df.witness_present_ind.fillna(0)\n",
    "test_df.witness_present_ind = test_df.witness_present_ind.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### liab_prct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['LiabBand'] = pd.cut(train_df['liab_prct'], 4)\n",
    "train_df[['LiabBand', 'fraud']].groupby(['LiabBand'], as_index=False).mean().sort_values(by='LiabBand', ascending=True)\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset.loc[ dataset['liab_prct'] <= 25, 'liab_prct'] = 0\n",
    "    dataset.loc[(dataset['liab_prct'] > 25) & (dataset['liab_prct'] <= 50), 'liab_prct'] = 1\n",
    "    dataset.loc[(dataset['liab_prct'] > 50) & (dataset['liab_prct'] <= 75), 'liab_prct'] = 2\n",
    "    dataset.loc[ dataset['liab_prct'] > 75, 'liab_prct'] = 3\n",
    "    dataset['liab_prct'] = dataset['liab_prct'].astype(int)\n",
    "\n",
    "train_df = train_df.drop(['LiabBand'], axis=1)\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.channel = train_df.channel.map({'Broker':1,'Phone':2,'Online':3})\n",
    "test_df.channel = test_df.channel.map({'Broker':1,'Phone':2,'Online':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### claim_est_payout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df.claim_est_payout > 100].claim_est_payout.mean()\n",
    "test_df[test_df.claim_est_payout > 100].claim_est_payout.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.claim_est_payout.fillna(4976,inplace=True)\n",
    "test_df.claim_est_payout.fillna(6758,inplace=True)\n",
    "\n",
    "train_df['claim_est_payout_band'] = pd.cut(train_df['claim_est_payout'], 5)\n",
    "train_df[['claim_est_payout_band', 'fraud']].groupby(['claim_est_payout_band'], as_index=False).mean().sort_values(by='claim_est_payout_band', ascending=True)\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset.loc[ dataset['claim_est_payout'] <= 3065, 'claim_est_payout'] = 0\n",
    "    dataset.loc[(dataset['claim_est_payout'] > 3065) & (dataset['claim_est_payout'] <= 4141), 'claim_est_payout'] = 1\n",
    "    dataset.loc[(dataset['claim_est_payout'] > 4141) & (dataset['claim_est_payout'] <= 5226), 'claim_est_payout'] = 2\n",
    "    dataset.loc[(dataset['claim_est_payout'] > 5226) & (dataset['claim_est_payout'] <= 6684), 'claim_est_payout'] = 3\n",
    "    dataset.loc[ dataset['claim_est_payout'] > 6684, 'claim_est_payout'] = 4\n",
    "    dataset['claim_est_payout'] = dataset['claim_est_payout'].astype(int)\n",
    "\n",
    "train_df = train_df.drop(['claim_est_payout_band'], axis=1)\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### age_of_vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.age_of_vehicle.describe()\n",
    "test_df.age_of_vehicle.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.age_of_vehicle.fillna(5,inplace=True)\n",
    "test_df.age_of_vehicle.fillna(5,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vehicle_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.vehicle_category = train_df.vehicle_category.map({'Compact':1,'Large':2,'Medium':3})\n",
    "test_df.vehicle_category = test_df.vehicle_category.map({'Compact':1,'Large':2,'Medium':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vehicle_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['vehicle_price_band'] = pd.cut(train_df['vehicle_price'], 7)\n",
    "train_df[['vehicle_price_band', 'fraud']].groupby(['vehicle_price_band'], as_index=False).mean().sort_values(by='vehicle_price_band', ascending=True)\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset.loc[ dataset['vehicle_price'] <= 20258, 'vehicle_price'] = 0\n",
    "    dataset.loc[(dataset['vehicle_price'] > 20258) & (dataset['vehicle_price'] <= 38059), 'vehicle_price'] = 1\n",
    "    dataset.loc[(dataset['vehicle_price'] > 38059) & (dataset['vehicle_price'] <= 55859), 'vehicle_price'] = 2\n",
    "    dataset.loc[(dataset['vehicle_price'] > 55859) & (dataset['vehicle_price'] <= 73660), 'vehicle_price'] = 3\n",
    "    dataset.loc[(dataset['vehicle_price'] > 73660) & (dataset['vehicle_price'] <= 91461), 'vehicle_price'] = 4\n",
    "    dataset.loc[(dataset['vehicle_price'] > 91461) & (dataset['vehicle_price'] <= 109262), 'vehicle_price'] = 5\n",
    "    dataset.loc[ dataset['vehicle_price'] > 109262, 'vehicle_price'] = 6\n",
    "    dataset['vehicle_price'] = dataset['vehicle_price'].astype(int)\n",
    "\n",
    "train_df = train_df.drop(['vehicle_price_band'], axis=1)\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vehicle_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.vehicle_color = train_df.vehicle_color.map({'black':1,'silver':2,'white':3,'red':4,'blue':5,'gray':6,'other':7})\n",
    "test_df.vehicle_color = test_df.vehicle_color.map({'black':1,'silver':2,'white':3,'red':4,'blue':5,'gray':6,'other':7})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vehicle_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['vehicle_weight_band'] = pd.cut(train_df['vehicle_weight'], 5)\n",
    "train_df[['vehicle_weight_band', 'fraud']].groupby(['vehicle_weight_band'], as_index=False).mean().sort_values(by='vehicle_weight_band', ascending=True)\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset.loc[ dataset['vehicle_weight'] <= 26546, 'vehicle_weight'] = 0\n",
    "    dataset.loc[(dataset['vehicle_weight'] > 26546) & (dataset['vehicle_weight'] <= 50664), 'vehicle_weight'] = 1\n",
    "    dataset.loc[(dataset['vehicle_weight'] > 50664) & (dataset['vehicle_weight'] <= 74781), 'vehicle_weight'] = 2\n",
    "    dataset.loc[(dataset['vehicle_weight'] > 74781) & (dataset['vehicle_weight'] <= 98899), 'vehicle_weight'] = 3\n",
    "    dataset.loc[ dataset['vehicle_weight'] > 98899, 'vehicle_weight'] = 4\n",
    "    dataset['vehicle_weight'] = dataset['vehicle_weight'].astype(int)\n",
    "\n",
    "train_df = train_df.drop(['vehicle_weight_band'], axis=1)\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Model, predict and solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop('fraud', axis=1)\n",
    "y = np.ravel(np.array(train_df[['fraud']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(1,31)\n",
    "para_grid = dict(n_neighbors=k_range)\n",
    "\n",
    "grid = GridSearchCV(knn, para_grid, cv=10, scoring='roc_auc',n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(knn, X, y, cv=10, scoring='roc_auc')\n",
    "print(scores)\n",
    "\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C=0.1, solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "para_grid = dict(C=C_range)\n",
    "\n",
    "grid = GridSearchCV(lr, para_grid, cv=10, scoring='roc_auc',n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2 = cross_val_score(lr, X, y, cv=10, scoring='roc_auc')\n",
    "print(scores2)\n",
    "\n",
    "np.mean(scores2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores3 =  cross_val_score(gnb, X, y, cv=10, scoring='roc_auc')\n",
    "print(scores3)\n",
    "\n",
    "np.mean(scores3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier(max_depth=7, min_samples_split=0.1, min_samples_leaf=0.1, max_features=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = range(1,31)\n",
    "para_grid = dict(max_depth=md)\n",
    "\n",
    "grid = GridSearchCV(dtc, para_grid, cv=10, scoring='roc_auc',n_jobs=-1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mss = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "para_grid = dict(min_samples_split=mss)\n",
    "\n",
    "grid = GridSearchCV(dtc, para_grid, cv=10, scoring='roc_auc',n_jobs=-1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msl = np.linspace(0.1, 0.5, 5, endpoint=True)\n",
    "para_grid = dict(min_samples_leaf=msl)\n",
    "\n",
    "grid = GridSearchCV(dtc, para_grid, cv=10, scoring='roc_auc',n_jobs=-1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = range(1, X.shape[1])\n",
    "para_grid = dict(max_features=mf)\n",
    "\n",
    "grid = GridSearchCV(dtc, para_grid, cv=10, scoring='roc_auc',n_jobs=-1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores4 =  cross_val_score(dtc, X, y, cv=10, scoring='roc_auc')\n",
    "print(scores4)\n",
    "\n",
    "np.mean(scores4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(max_features=16, max_depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = range(200,2200,200)\n",
    "para_grid = dict(n_estimators=n_estimators)\n",
    "\n",
    "grid = GridSearchCV(rfc, para_grid, cv=10, scoring='roc_auc',n_jobs=-1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = range(1, X.shape[1])\n",
    "para_grid = dict(max_features=mf)\n",
    "\n",
    "grid = GridSearchCV(rfc, para_grid, cv=10, scoring='roc_auc',n_jobs=-1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = range(1,31)\n",
    "para_grid = dict(max_depth=md)\n",
    "\n",
    "grid = GridSearchCV(rfc, para_grid, cv=10, scoring='roc_auc',n_jobs=-1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores5 =  cross_val_score(rfc, X, y, cv=10, scoring='roc_auc')\n",
    "print(scores5)\n",
    "\n",
    "np.mean(scores5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = [0.01, 0.1, 1, 10, 100]\n",
    "para_grid = dict(learning_rate=lr)\n",
    "\n",
    "grid = GridSearchCV(gbc, para_grid, cv=10, scoring='roc_auc',n_jobs=-1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores6 =  cross_val_score(gbc, X, y, cv=10, scoring='roc_auc')\n",
    "print(scores6)\n",
    "\n",
    "np.mean(scores6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc.fit(X_scaled,y)\n",
    "print(gbc.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(gbc.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "feat_importances.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = ['relu', 'logistic', 'tanh']\n",
    "para_grid = dict(activation=act)\n",
    "\n",
    "grid = GridSearchCV(mlp, para_grid, cv=10, scoring='roc_auc',n_jobs=-1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores7 =  cross_val_score(gbc, X, y, cv=10, scoring='roc_auc')\n",
    "print(scores7)\n",
    "\n",
    "np.mean(scores7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 2,\n",
    "    'learning_rate': 1.0,\n",
    "    'silent': 1.0,\n",
    "    'n_estimators': 5\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores8 =  cross_val_score(xgb, X, y, cv=10, scoring='roc_auc')\n",
    "print(scores8)\n",
    "\n",
    "np.mean(scores8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['KNN', 'Logistic Regression'\n",
    "              , 'Naive Bayes', 'Decision Tree', \n",
    "              'Random Forest', 'Gradient Bossted Decision Trees', \n",
    "              'Neural Network', 'XGBoost'],\n",
    "    'Score': [np.mean(scores), np.mean(scores2), np.mean(scores3), np.mean(scores4), np.mean(scores5), np.mean(scores6), np.mean(scores7), np.mean(scores8)]})\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
